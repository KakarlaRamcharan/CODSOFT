import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
base_model = ResNet50(weights='imagenet')
base_model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)
def extract_features(image_path):
    img = image.load_img(image_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    features = base_model.predict(x)
    return features
embeddings_index = {}
with open('glove.6B.200d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
max_sequence_length = 20
vocab_size = len(embeddings_index)
def generate_caption(image_features):
    inputs = np.zeros((1, max_sequence_length))
    caption = ['<start>']
    for i in range(max_sequence_length - 1):
        seq = [embeddings_index[word] for word in caption if word in embeddings_index]
        seq = pad_sequences([seq], maxlen=max_sequence_length, padding='post', truncating='post')
        inputs[:, i] = seq[:, i]
        preds = model.predict([image_features, inputs])
        word_idx = np.argmax(preds[0, i])
        word = [word for word, idx in embeddings_index.items() if np.array_equal(idx, word_idx)][0]
        if word == '<end>':
            break
        caption.append(word)   
    return ' '.join(caption[1:-1])
image_input = tf.keras.layers.Input(shape=(2048,))
image_dense = Dense(256, activation='relu')(image_input)
caption_input = tf.keras.layers.Input(shape=(max_sequence_length,))
caption_embedding = Embedding(input_dim=vocab_size, output_dim=200)(caption_input)
caption_lstm = LSTM(256)(caption_embedding)
decoder_input = tf.keras.layers.Concatenate()([image_dense, caption_lstm])
output = Dense(vocab_size, activation='softmax')(decoder_input)
model = Model(inputs=[image_input, caption_input], outputs=output)
image_path = 'test_image.jpg'
image_features = extract_features(image_path)
caption = generate_caption(image_features)                                 
print(caption)
